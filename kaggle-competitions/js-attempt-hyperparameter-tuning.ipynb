{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/jane-street-real-time-market-data-forecasting'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcbt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from optuna import Trial, create_study\n",
    "import gc\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:16:16.823146Z",
     "iopub.status.busy": "2025-01-15T21:16:16.822879Z",
     "iopub.status.idle": "2025-01-15T21:16:16.838085Z",
     "shell.execute_reply": "2025-01-15T21:16:16.837157Z",
     "shell.execute_reply.started": "2025-01-15T21:16:16.823125Z"
    }
   },
   "outputs": [],
   "source": [
    "class PurgedKFold:\n",
    "    def __init__(self, n_splits=5, purge_window=20, embargo_pct=0.02):\n",
    "        self.n_splits = n_splits\n",
    "        self.purge_window = purge_window\n",
    "        self.embargo_pct = embargo_pct\n",
    "    \n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        embargo_size = int(self.embargo_pct * n_samples)\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_splits)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            test_start = test_idx[0]\n",
    "            test_end = test_idx[-1]\n",
    "            \n",
    "            # Remove samples within purge window\n",
    "            purge_start = max(0, test_start - self.purge_window)\n",
    "            purge_end = min(n_samples, test_end + self.purge_window + embargo_size)\n",
    "            \n",
    "            train_mask = np.ones(n_samples, dtype=bool)\n",
    "            train_mask[purge_start:purge_end] = False\n",
    "            \n",
    "            final_train_idx = indices[train_mask & np.isin(indices, train_idx)]\n",
    "            \n",
    "            yield final_train_idx, test_idx\n",
    "\n",
    "def load_data(input_path, n_partitions=3):\n",
    "    \"\"\"Load and combine data from multiple partitions\"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    for partition_id in range(n_partitions):\n",
    "        partition_path = f'{input_path}/train.parquet/partition_id={partition_id}/part-0.parquet'\n",
    "        df = pd.read_parquet(partition_path)\n",
    "        df_list.append(df)\n",
    "        gc.collect()\n",
    "    \n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    del df_list\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def objective_lgb(trial: Trial, X_train, y_train, X_valid, y_valid, w_train, w_valid):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 255),  # broader range\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),  # increased upper bound\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 7),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 8),  # narrower, more reasonable range\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),  # lower range\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),  # less extreme range\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),  # less extreme range\n",
    "        'device': 'gpu',\n",
    "        'gpu_use_dp': True,\n",
    "        'n_estimators': 1000\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    \n",
    "    # Use callbacks without verbose parameter\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='rmse',\n",
    "        eval_sample_weight=[w_valid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(np.average((preds - y_valid) ** 2, weights=w_valid))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def objective_xgb(trial: Trial, X_train, y_train, X_valid, y_valid, w_train, w_valid):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'tree_method': 'hist',  # Changed from 'hist' to 'gpu_hist'\n",
    "        'device': 'cuda',  # Explicitly specify CUDA device\n",
    "        'early_stopping_rounds': 100  # Added here instead of in fit\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    \n",
    "    # Convert data to GPU\n",
    "    dtrain = xgb.DMatrix(X_train, y_train, weight=w_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid, weight=w_valid)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        sample_weight_eval_set=[w_valid],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(np.average((preds - y_valid) ** 2, weights=w_valid))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def objective_catboost(trial: Trial, X_train, y_train, X_valid, y_valid, w_train, w_valid):\n",
    "    param = {\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'task_type': 'GPU',\n",
    "        'loss_function': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    model = cbt.CatBoostRegressor(**param)\n",
    "    eval_pool = cbt.Pool(X_valid, y_valid, weight=w_valid)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[eval_pool],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(np.average((preds - y_valid) ** 2, weights=w_valid))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-15T21:23:17.400Z",
     "iopub.execute_input": "2025-01-15T21:16:17.687617Z",
     "iopub.status.busy": "2025-01-15T21:16:17.687319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-15 21:16:22,707] A new study created in memory with name: no-name-c5e4494f-f99b-4a43-b89a-9aadc49bd841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing xgb...\n",
      "Processing fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-15 21:17:50,654] Trial 0 finished with value: 0.7839458584785461 and parameters: {'max_depth': 5, 'learning_rate': 0.04316606815265267, 'min_child_weight': 90, 'subsample': 0.9552442007331393, 'colsample_bytree': 0.6593456004656606, 'gamma': 0.010853096969235122, 'reg_alpha': 5.532685262070338e-06, 'reg_lambda': 2.7406194109816155}. Best is trial 0 with value: 0.7839458584785461.\n",
      "[I 2025-01-15 21:19:16,076] Trial 1 finished with value: 0.783700168132782 and parameters: {'max_depth': 7, 'learning_rate': 0.031202646654055047, 'min_child_weight': 13, 'subsample': 0.6937766181023162, 'colsample_bytree': 0.8617708545910219, 'gamma': 0.10433095804462972, 'reg_alpha': 0.06296985190532312, 'reg_lambda': 1.7426482313531487e-08}. Best is trial 1 with value: 0.783700168132782.\n",
      "[I 2025-01-15 21:20:38,150] Trial 2 finished with value: 0.7837851047515869 and parameters: {'max_depth': 5, 'learning_rate': 0.05188594450434426, 'min_child_weight': 10, 'subsample': 0.9993476725724016, 'colsample_bytree': 0.7468061306593282, 'gamma': 0.026498649149038513, 'reg_alpha': 0.055257087090904114, 'reg_lambda': 1.7753413998966864e-08}. Best is trial 1 with value: 0.783700168132782.\n",
      "[I 2025-01-15 21:22:12,564] Trial 3 finished with value: 0.7842831015586853 and parameters: {'max_depth': 12, 'learning_rate': 0.08693042674987007, 'min_child_weight': 71, 'subsample': 0.6147965958280167, 'colsample_bytree': 0.7233538044297101, 'gamma': 9.640577329435127e-07, 'reg_alpha': 1.0248012400773178e-07, 'reg_lambda': 0.0196726005678896}. Best is trial 1 with value: 0.783700168132782.\n",
      "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x7eb995bcff70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 589, in _next_wrapper\n",
      "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set paths\n",
    "    input_path = '/kaggle/input/jane-street-real-time-market-data-forecasting'\n",
    "    output_path = 'model_params'\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Feature names\n",
    "    feature_names = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(input_path, n_partitions=3)  # Adjust n_partitions as needed\n",
    "    \n",
    "    # Initialize PurgedKFold\n",
    "    cv = PurgedKFold(n_splits=5, purge_window=20, embargo_pct=0.02)\n",
    "    \n",
    "    # Dictionary to store results for each model\n",
    "    results = {}\n",
    "    \n",
    "    # For each model type\n",
    "    #for model_type in ['lgb', 'xgb', 'catboost']:\n",
    "    for model_type in [ 'xgb', 'catboost']:\n",
    "        print(f\"\\nOptimizing {model_type}...\")\n",
    "        \n",
    "        # Create study\n",
    "        study = create_study(direction='minimize')\n",
    "        fold_scores = []\n",
    "        \n",
    "        # Select objective function\n",
    "        if model_type == 'lgb':\n",
    "            objective = objective_lgb\n",
    "        elif model_type == 'xgb':\n",
    "            objective = objective_xgb\n",
    "        else:\n",
    "            objective = objective_catboost\n",
    "        \n",
    "        # Run optimization for each fold\n",
    "        for fold, (train_idx, valid_idx) in enumerate(cv.split(df[feature_names])):\n",
    "            print(f\"Processing fold {fold + 1}\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train = df[feature_names].iloc[train_idx]\n",
    "            X_valid = df[feature_names].iloc[valid_idx]\n",
    "            y_train = df['responder_6'].iloc[train_idx]\n",
    "            y_valid = df['responder_6'].iloc[valid_idx]\n",
    "            w_train = df['weight'].iloc[train_idx]\n",
    "            w_valid = df['weight'].iloc[valid_idx]\n",
    "            \n",
    "            # Create fold-specific objective\n",
    "            fold_objective = lambda trial: objective(\n",
    "                trial, X_train, y_train, X_valid, y_valid, w_train, w_valid\n",
    "            )\n",
    "            \n",
    "            # Optimize\n",
    "            study.optimize(fold_objective, n_trials=20)  # Adjust n_trials as needed\n",
    "            fold_scores.append(study.best_value)\n",
    "            \n",
    "            # Clean up memory\n",
    "            gc.collect()\n",
    "        \n",
    "        # Store results\n",
    "        results[model_type] = {\n",
    "            'best_params': study.best_params,\n",
    "            'mean_rmse': float(np.mean(fold_scores)),\n",
    "            'std_rmse': float(np.std(fold_scores)),\n",
    "            'fold_scores': [float(score) for score in fold_scores]\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{model_type.upper()} Results:\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        print(f\"Mean RMSE: {np.mean(fold_scores):.6f}\")\n",
    "        print(f\"Std RMSE: {np.std(fold_scores):.6f}\")\n",
    "        \n",
    "        # Save individual model results\n",
    "        with open(f'{output_path}/{model_type}_results.json', 'w') as f:\n",
    "            json.dump(results[model_type], f, indent=4)\n",
    "    \n",
    "    # Save all results\n",
    "    with open(f'{output_path}/all_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(\"\\nOptimization completed. Results saved in 'model_params' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
